{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro-hw01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_upCOEI3Upu"
      },
      "source": [
        "# Основы глубинного обучения, майнор ИАД\n",
        "\n",
        "## Домашнее задание 1. Введение в PyTorch. Полносвязные нейронные сети.\n",
        "\n",
        "### Общая информация\n",
        "\n",
        "Дата выдачи: 06.10.2021\n",
        "\n",
        "Мягкий дедлайн: 23:59MSK 25.10.2021\n",
        "\n",
        "Жесткий дедлайн: 23:59MSK 28.10.2021\n",
        "\n",
        "### Оценивание и штрафы\n",
        "Максимально допустимая оценка за работу — 10 баллов. За каждый день просрочки снимается 1 балл. Сдавать задание после жёсткого дедлайна сдачи нельзя.\n",
        "\n",
        "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
        "\n",
        "Неэффективная реализация кода может негативно отразиться на оценке.\n",
        "Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
        "\n",
        "### О задании\n",
        "\n",
        "В этом задании вам предстоит предсказывать год выпуска песни по некоторым звуковым признакам: [данные](https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd). В ячейках ниже находится код для загрузки данных. Обратите внимание, что обучающая и тестовая выборки располагаются в одном файле, поэтому НЕ меняйте ячейку, в которой производится деление данных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI_eoe063VaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be6c4642-360d-48d4-b800-dc1fe031dfd5"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NgSZeU-7vgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6370509-efee-45ef-ce7f-c09af2d2dd3c"
      },
      "source": [
        "!wget -O data.txt.zip https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-17 18:54:15--  https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 211011981 (201M) [application/x-httpd-php]\n",
            "Saving to: ‘data.txt.zip’\n",
            "\n",
            "data.txt.zip        100%[===================>] 201.24M  95.9MB/s    in 2.1s    \n",
            "\n",
            "2021-10-17 18:54:17 (95.9 MB/s) - ‘data.txt.zip’ saved [211011981/211011981]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSVJZzkJ7zZE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "eed9e8dd-ba7b-436d-c7d5-c005d379032e"
      },
      "source": [
        "df = pd.read_csv('data.txt.zip', header=None)\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>-2.46783</td>\n",
              "      <td>3.32136</td>\n",
              "      <td>-2.31521</td>\n",
              "      <td>10.20556</td>\n",
              "      <td>611.10913</td>\n",
              "      <td>951.08960</td>\n",
              "      <td>698.11428</td>\n",
              "      <td>408.98485</td>\n",
              "      <td>383.70912</td>\n",
              "      <td>326.51512</td>\n",
              "      <td>238.11327</td>\n",
              "      <td>251.42414</td>\n",
              "      <td>187.17351</td>\n",
              "      <td>100.42652</td>\n",
              "      <td>179.19498</td>\n",
              "      <td>-8.41558</td>\n",
              "      <td>-317.87038</td>\n",
              "      <td>95.86266</td>\n",
              "      <td>48.10259</td>\n",
              "      <td>-95.66303</td>\n",
              "      <td>-18.06215</td>\n",
              "      <td>1.96984</td>\n",
              "      <td>34.42438</td>\n",
              "      <td>11.72670</td>\n",
              "      <td>1.36790</td>\n",
              "      <td>7.79444</td>\n",
              "      <td>-0.36994</td>\n",
              "      <td>-133.67852</td>\n",
              "      <td>-83.26165</td>\n",
              "      <td>-37.29765</td>\n",
              "      <td>...</td>\n",
              "      <td>-25.38187</td>\n",
              "      <td>-3.90772</td>\n",
              "      <td>13.29258</td>\n",
              "      <td>41.55060</td>\n",
              "      <td>-7.26272</td>\n",
              "      <td>-21.00863</td>\n",
              "      <td>105.50848</td>\n",
              "      <td>64.29856</td>\n",
              "      <td>26.08481</td>\n",
              "      <td>-44.59110</td>\n",
              "      <td>-8.30657</td>\n",
              "      <td>7.93706</td>\n",
              "      <td>-10.73660</td>\n",
              "      <td>-95.44766</td>\n",
              "      <td>-82.03307</td>\n",
              "      <td>-35.59194</td>\n",
              "      <td>4.69525</td>\n",
              "      <td>70.95626</td>\n",
              "      <td>28.09139</td>\n",
              "      <td>6.02015</td>\n",
              "      <td>-37.13767</td>\n",
              "      <td>-41.12450</td>\n",
              "      <td>-8.40816</td>\n",
              "      <td>7.19877</td>\n",
              "      <td>-8.60176</td>\n",
              "      <td>-5.90857</td>\n",
              "      <td>-12.32437</td>\n",
              "      <td>14.68734</td>\n",
              "      <td>-54.32125</td>\n",
              "      <td>40.14786</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>4.59210</td>\n",
              "      <td>2.21920</td>\n",
              "      <td>0.34006</td>\n",
              "      <td>44.38997</td>\n",
              "      <td>2056.93836</td>\n",
              "      <td>605.40696</td>\n",
              "      <td>457.41175</td>\n",
              "      <td>777.15347</td>\n",
              "      <td>415.64880</td>\n",
              "      <td>746.47775</td>\n",
              "      <td>366.45320</td>\n",
              "      <td>317.82946</td>\n",
              "      <td>273.07917</td>\n",
              "      <td>141.75921</td>\n",
              "      <td>317.35269</td>\n",
              "      <td>19.48271</td>\n",
              "      <td>-65.25496</td>\n",
              "      <td>162.75145</td>\n",
              "      <td>135.00765</td>\n",
              "      <td>-96.28436</td>\n",
              "      <td>-86.87955</td>\n",
              "      <td>17.38087</td>\n",
              "      <td>45.90742</td>\n",
              "      <td>32.49908</td>\n",
              "      <td>-32.85429</td>\n",
              "      <td>45.10830</td>\n",
              "      <td>26.84939</td>\n",
              "      <td>-302.57328</td>\n",
              "      <td>-41.71932</td>\n",
              "      <td>-138.85034</td>\n",
              "      <td>...</td>\n",
              "      <td>28.55107</td>\n",
              "      <td>1.52298</td>\n",
              "      <td>70.99515</td>\n",
              "      <td>-43.63073</td>\n",
              "      <td>-42.55014</td>\n",
              "      <td>129.82848</td>\n",
              "      <td>79.95420</td>\n",
              "      <td>-87.14554</td>\n",
              "      <td>-45.75446</td>\n",
              "      <td>-65.82100</td>\n",
              "      <td>-43.90031</td>\n",
              "      <td>-19.45705</td>\n",
              "      <td>12.59163</td>\n",
              "      <td>-407.64130</td>\n",
              "      <td>42.91189</td>\n",
              "      <td>12.15850</td>\n",
              "      <td>-88.37882</td>\n",
              "      <td>42.25246</td>\n",
              "      <td>46.49209</td>\n",
              "      <td>-30.17747</td>\n",
              "      <td>45.98495</td>\n",
              "      <td>130.47892</td>\n",
              "      <td>13.88281</td>\n",
              "      <td>-4.00055</td>\n",
              "      <td>17.85965</td>\n",
              "      <td>-18.32138</td>\n",
              "      <td>-87.99109</td>\n",
              "      <td>14.37524</td>\n",
              "      <td>-22.70119</td>\n",
              "      <td>-58.81266</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>1.39518</td>\n",
              "      <td>2.73553</td>\n",
              "      <td>0.82804</td>\n",
              "      <td>7.46586</td>\n",
              "      <td>699.54544</td>\n",
              "      <td>1016.00954</td>\n",
              "      <td>594.06748</td>\n",
              "      <td>355.73663</td>\n",
              "      <td>507.39931</td>\n",
              "      <td>387.69910</td>\n",
              "      <td>287.15347</td>\n",
              "      <td>112.37152</td>\n",
              "      <td>161.68928</td>\n",
              "      <td>144.14353</td>\n",
              "      <td>199.29693</td>\n",
              "      <td>-4.24359</td>\n",
              "      <td>-297.00587</td>\n",
              "      <td>-148.36392</td>\n",
              "      <td>-7.94726</td>\n",
              "      <td>-18.71630</td>\n",
              "      <td>12.77542</td>\n",
              "      <td>-25.37725</td>\n",
              "      <td>9.71410</td>\n",
              "      <td>0.13843</td>\n",
              "      <td>26.79723</td>\n",
              "      <td>6.30760</td>\n",
              "      <td>28.70107</td>\n",
              "      <td>-74.89005</td>\n",
              "      <td>-289.19553</td>\n",
              "      <td>-166.26089</td>\n",
              "      <td>...</td>\n",
              "      <td>18.50939</td>\n",
              "      <td>16.97216</td>\n",
              "      <td>24.26629</td>\n",
              "      <td>-10.50788</td>\n",
              "      <td>-8.68412</td>\n",
              "      <td>54.75759</td>\n",
              "      <td>194.74034</td>\n",
              "      <td>7.95966</td>\n",
              "      <td>-18.22685</td>\n",
              "      <td>0.06463</td>\n",
              "      <td>-2.63069</td>\n",
              "      <td>26.02561</td>\n",
              "      <td>1.75729</td>\n",
              "      <td>-262.36917</td>\n",
              "      <td>-233.60089</td>\n",
              "      <td>-2.50502</td>\n",
              "      <td>-12.14279</td>\n",
              "      <td>81.37617</td>\n",
              "      <td>2.07554</td>\n",
              "      <td>-1.82381</td>\n",
              "      <td>183.65292</td>\n",
              "      <td>22.64797</td>\n",
              "      <td>-39.98887</td>\n",
              "      <td>43.37381</td>\n",
              "      <td>-31.56737</td>\n",
              "      <td>-4.88840</td>\n",
              "      <td>-36.53213</td>\n",
              "      <td>-23.94662</td>\n",
              "      <td>-84.19275</td>\n",
              "      <td>66.00518</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>-6.36304</td>\n",
              "      <td>6.63016</td>\n",
              "      <td>-3.35142</td>\n",
              "      <td>37.64085</td>\n",
              "      <td>2174.08189</td>\n",
              "      <td>697.43346</td>\n",
              "      <td>459.24587</td>\n",
              "      <td>742.78961</td>\n",
              "      <td>229.30783</td>\n",
              "      <td>387.89697</td>\n",
              "      <td>249.06662</td>\n",
              "      <td>245.89870</td>\n",
              "      <td>176.20527</td>\n",
              "      <td>98.82222</td>\n",
              "      <td>150.97286</td>\n",
              "      <td>78.49057</td>\n",
              "      <td>-62.00282</td>\n",
              "      <td>43.49659</td>\n",
              "      <td>-96.42719</td>\n",
              "      <td>-108.96608</td>\n",
              "      <td>14.22854</td>\n",
              "      <td>14.54178</td>\n",
              "      <td>-23.55608</td>\n",
              "      <td>-39.36953</td>\n",
              "      <td>-43.59209</td>\n",
              "      <td>20.83714</td>\n",
              "      <td>35.63919</td>\n",
              "      <td>-181.34947</td>\n",
              "      <td>-93.66614</td>\n",
              "      <td>-90.55616</td>\n",
              "      <td>...</td>\n",
              "      <td>4.56917</td>\n",
              "      <td>-37.32280</td>\n",
              "      <td>4.15159</td>\n",
              "      <td>12.24315</td>\n",
              "      <td>35.02697</td>\n",
              "      <td>-178.89573</td>\n",
              "      <td>82.46573</td>\n",
              "      <td>-20.49425</td>\n",
              "      <td>101.78577</td>\n",
              "      <td>-19.77808</td>\n",
              "      <td>-21.52657</td>\n",
              "      <td>3.36303</td>\n",
              "      <td>-11.63176</td>\n",
              "      <td>51.55411</td>\n",
              "      <td>-50.57576</td>\n",
              "      <td>-28.14755</td>\n",
              "      <td>-83.15795</td>\n",
              "      <td>-7.35260</td>\n",
              "      <td>-22.11505</td>\n",
              "      <td>1.18279</td>\n",
              "      <td>-122.70467</td>\n",
              "      <td>150.57360</td>\n",
              "      <td>24.37468</td>\n",
              "      <td>41.19821</td>\n",
              "      <td>-37.04318</td>\n",
              "      <td>-28.72986</td>\n",
              "      <td>162.19614</td>\n",
              "      <td>22.18309</td>\n",
              "      <td>-8.63509</td>\n",
              "      <td>85.23416</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>0.93609</td>\n",
              "      <td>1.60923</td>\n",
              "      <td>2.19223</td>\n",
              "      <td>47.32082</td>\n",
              "      <td>894.28471</td>\n",
              "      <td>809.86615</td>\n",
              "      <td>318.78559</td>\n",
              "      <td>435.04497</td>\n",
              "      <td>341.61467</td>\n",
              "      <td>334.30734</td>\n",
              "      <td>322.99589</td>\n",
              "      <td>190.61921</td>\n",
              "      <td>235.84715</td>\n",
              "      <td>96.89517</td>\n",
              "      <td>210.58870</td>\n",
              "      <td>5.60463</td>\n",
              "      <td>-199.63958</td>\n",
              "      <td>204.85812</td>\n",
              "      <td>-77.17695</td>\n",
              "      <td>-65.79741</td>\n",
              "      <td>-6.95097</td>\n",
              "      <td>-12.15262</td>\n",
              "      <td>-3.85410</td>\n",
              "      <td>20.68990</td>\n",
              "      <td>-20.30480</td>\n",
              "      <td>37.15045</td>\n",
              "      <td>11.20673</td>\n",
              "      <td>-124.09519</td>\n",
              "      <td>-295.98542</td>\n",
              "      <td>-33.31169</td>\n",
              "      <td>...</td>\n",
              "      <td>45.25506</td>\n",
              "      <td>10.42226</td>\n",
              "      <td>27.88782</td>\n",
              "      <td>-17.12676</td>\n",
              "      <td>-31.54772</td>\n",
              "      <td>-76.86293</td>\n",
              "      <td>41.17343</td>\n",
              "      <td>-138.32535</td>\n",
              "      <td>-53.96905</td>\n",
              "      <td>-21.30266</td>\n",
              "      <td>-24.87362</td>\n",
              "      <td>-2.46595</td>\n",
              "      <td>-4.05003</td>\n",
              "      <td>-56.51161</td>\n",
              "      <td>-34.56445</td>\n",
              "      <td>-5.07092</td>\n",
              "      <td>-47.75605</td>\n",
              "      <td>64.81513</td>\n",
              "      <td>-97.42948</td>\n",
              "      <td>-12.59418</td>\n",
              "      <td>55.23699</td>\n",
              "      <td>28.85657</td>\n",
              "      <td>54.53513</td>\n",
              "      <td>-31.97077</td>\n",
              "      <td>20.03279</td>\n",
              "      <td>-8.07892</td>\n",
              "      <td>-55.12617</td>\n",
              "      <td>26.58961</td>\n",
              "      <td>-10.27183</td>\n",
              "      <td>-30.64232</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 91 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0         1         2         3   ...         87        88         89        90\n",
              "0  2001  49.94357  21.47114  73.07750  ...   68.40795  -1.82223  -27.46348   2.26327\n",
              "1  2001  48.73215  18.42930  70.32679  ...   70.49388  12.04941   58.43453  26.92061\n",
              "2  2001  50.95714  31.85602  55.81851  ... -115.00698  -0.05859   39.67068  -0.66345\n",
              "3  2001  48.24750  -1.89837  36.29772  ...  -72.08993   9.90558  199.62971  18.85382\n",
              "4  2001  50.97020  42.20998  67.09964  ...   51.76631   7.88713   55.66926  28.74903\n",
              "\n",
              "[5 rows x 91 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4wnRJT1778j"
      },
      "source": [
        "X = df.iloc[:, 1:].values\n",
        "y = df.iloc[:, 0].values\n",
        "\n",
        "train_size = 463715\n",
        "X_train = X[:train_size, :]\n",
        "y_train = y[:train_size]\n",
        "X_test = X[train_size:, :]\n",
        "y_test = y[train_size:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_386JE_o5gOd"
      },
      "source": [
        "## Задание 0. (0 баллов, но при невыполнении максимум за все задание &mdash; 0 баллов)\n",
        "\n",
        "Мы будем использовать RMSE как метрику качества. Для самого первого бейзлайна обучите `Ridge` регрессию из `sklearn`. Кроме того, посчитайте качество при наилучшем константном прогнозе."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt9JbIphJnvh"
      },
      "source": [
        "**Ridge регрессия**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otwuisa56MLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebee5f53-7af6-43ef-e85f-afda0aef7a7a"
      },
      "source": [
        "\n",
        "# YOUR CODE HERE (－.－)...zzzZZZzzzZZZ\n",
        "from sklearn import linear_model\n",
        "\n",
        "clf_ridge = linear_model.Ridge(alpha=2)\n",
        "clf_ridge.fit(X_train, y_train)\n",
        "\n",
        "print(\"Среднеквадратичная ошибка на тренировочной выборке\")\n",
        "print(np.sqrt(np.mean((clf_ridge.predict(X_train) -  y_train) ** 2)))\n",
        "\n",
        "print(\"Среднеквадратичная ошибка на тестовой выборке\")\n",
        "print(np.sqrt(np.mean((clf_ridge.predict(X_test) -  y_test) ** 2)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Среднеквадратичная ошибка на тренировочной выборке\n",
            "9.552822967904117\n",
            "Среднеквадратичная ошибка на тестовой выборке\n",
            "9.510160715258205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6TqedlUJwTb"
      },
      "source": [
        "**Константный прогноз**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4ScQGHzJ18S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e6c84b-2589-4384-e440-f592512b52ee"
      },
      "source": [
        "X_train_zero = np.zeros_like(X_train)\n",
        "X_test_zero = np.zeros_like(X_test)\n",
        "constant_clf_ridge = linear_model.Ridge(alpha=2)\n",
        "constant_clf_ridge.fit(X_train_zero, y_train)\n",
        "\n",
        "print(\"Лучший константный прогноз y =\", constant_clf_ridge.intercept_)\n",
        "print(\"Среднеквадратичная ошибка на тренировочной выборке\")\n",
        "print(np.sqrt(np.mean((constant_clf_ridge.predict(X_train_zero) -  y_train) ** 2)))\n",
        "\n",
        "print(\"Среднеквадратичная ошибка на тестовой выборке\")\n",
        "print(np.sqrt(np.mean((constant_clf_ridge.predict(X_test_zero) -  y_test) ** 2)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучший константный прогноз y = 1998.3860949074324\n",
            "Среднеквадратичная ошибка на тренировочной выборке\n",
            "10.939755150678016\n",
            "Среднеквадратичная ошибка на тестовой выборке\n",
            "10.85246390513634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE6AklNMNzXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95fe4aa-7f08-4a88-ff64-469b940519ad"
      },
      "source": [
        "np.mean(y_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1998.3860949074324"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ilBKYt6OdD"
      },
      "source": [
        "## Задание 1. (максимум 10 баллов)\n",
        "\n",
        "Реализуйте обучение и тестирование нейронной сети для предоставленного вам набора данных. Соотношение между полученным значением метрики на тестовой выборке и баллами за задание следующее:\n",
        "\n",
        "- $\\text{RMSE} \\le 9.00 $ &mdash; 4 балла\n",
        "- $\\text{RMSE} \\le 8.90 $ &mdash; 6 баллов\n",
        "- $\\text{RMSE} \\le 8.80 $ &mdash; 8 баллов\n",
        "- $\\text{RMSE} \\le 8.75 $ &mdash; 10 баллов\n",
        "\n",
        "Есть несколько правил, которых вам нужно придерживаться:\n",
        "\n",
        "- Весь пайплайн обучения должен быть написан на PyTorch. При этом вы можете пользоваться другими библиотеками (`numpy`, `sklearn` и пр.), но только для обработки данных. То есть как угодно трансформировать данные и считать метрики с помощью этих библиотек можно, а импортировать модели из `sklearn` и выбивать с их помощью требуемое качество &mdash; нельзя. Также нельзя пользоваться библиотеками, для которых сам PyTorch является зависимостью.\n",
        "\n",
        "- Мы никак не ограничиваем ваш выбор архитектуры модели, но скорее всего вам будет достаточно полносвязной нейронной сети.\n",
        "\n",
        "- Для обучения запрещается использовать какие-либо иные данные, кроме обучающей выборки.\n",
        "\n",
        "- Ансамблирование моделей запрещено.\n",
        "\n",
        "### Полезные советы:\n",
        "\n",
        "- Очень вряд ли, что у вас с первого раза получится выбить качество на 10 баллов, поэтому пробуйте разные архитектуры, оптимизаторы и значения гиперпараметров. В идеале при запуске каждого нового эксперимента вы должны менять что-то одно, чтобы точно знать, как этот фактор влияет на качество.\n",
        "\n",
        "- Тот факт, что мы занимаемся глубинным обучением, не означает, что стоит забывать про приемы, использующиеся в классическом машинном обучении. Так что обязательно проводите исследовательский анализ данных, отрисовывайте нужные графики и не забывайте про масштабирование и подбор гиперпараметров.\n",
        "\n",
        "- Вы наверняка столкнетесь с тем, что ваша нейронная сеть будет сильно переобучаться. Для нейросетей существуют специальные методы регуляризации, например, dropout ([статья](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)) и weight decay ([блогпост](https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd)). Они, разумеется, реализованы в PyTorch. Попробуйте поэкспериментировать с ними.\n",
        "\n",
        "- Если вы чего-то не знаете, не гнушайтесь гуглить. В интернете очень много полезной информации, туториалов и советов по глубинному обучению в целом и по PyTorch в частности. Но не забывайте, что за скатанный код без ссылки на источник придется ответить по всей строгости!\n",
        "\n",
        "- Если вы сразу реализуете обучение на GPU, то у вас будет больше времени на эксперименты, так как любые вычисления будут работать быстрее. Google Colab предоставляет несколько GPU-часов (обычно около 8-10) в сутки бесплатно.\n",
        "\n",
        "- Чтобы отладить код, можете обучаться на небольшой части данных или даже на одном батче. Если лосс на обучающей выборке не падает, то что-то точно идет не так!\n",
        "\n",
        "- Пользуйтесь утилитами, которые вам предоставляет PyTorch (например, Dataset и Dataloader). Их специально разработали для упрощения разработки пайплайна обучения.\n",
        "\n",
        "- Скорее всего вы захотите отслеживать прогресс обучения. Для создания прогресс-баров есть удобная библиотека `tqdm`.\n",
        "\n",
        "- Быть может, вы захотите, чтобы графики рисовались прямо во время обучения. Можете воспользоваться функцией [clear_output](http://ipython.org/ipython-doc/dev/api/generated/IPython.display.html#IPython.display.clear_output), чтобы удалять старый график и рисовать новый на его месте.\n",
        "\n",
        "**ОБЯЗАТЕЛЬНО** рисуйте графики зависимости лосса/метрики на обучающей и тестовой выборках в зависимости от времени обучения. Если обучение занимает относительно небольшое число эпох, то лучше рисовать зависимость от номера шага обучения, если же эпох больше, то рисуйте зависимость по эпохам. Если проверяющий не увидит такого графика для вашей лучшей модели, то он в праве снизить баллы за задание.\n",
        "\n",
        "**ВАЖНО!** Ваше решение должно быть воспроизводимым. Если это не так, то проверяющий имеет право снизить баллы за задание. Чтобы зафиксировать random seed, воспользуйтесь функцией из ячейки ниже.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaMButDmEKKw"
      },
      "source": [
        "def set_random_seed(seed):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZW0gMe3vT8u"
      },
      "source": [
        "Вы можете придерживаться любой адекватной струкуры кода, но мы советуем воспользоваться следующими сигнатурами функций. Лучше всего, если вы проверите ваши предсказания ассертом: так вы убережете себя от разных косяков, например, что вектор предсказаний состоит из всего одного числа. В любом случае, внимательно следите за тем, для каких тензоров вы считаете метрику RMSE. При случайном или намеренном введении в заблуждение проверяющие очень сильно разозлятся."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLdjObriB86H"
      },
      "source": [
        "# Эксперимент 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARjsURM2B8nR",
        "outputId": "31b6cc83-ae66-45fc-ff5e-a7ca57017b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "%matplotlib inline\n",
        " \n",
        " \n",
        "def set_random_seed(seed):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        " \n",
        "set_random_seed(42)\n",
        " \n",
        " \n",
        "class BaseDataset(Dataset):\n",
        "    def __init__(self, X_train, y_train, X_test, y_test, is_train=True):\n",
        "        self.scaler_ = StandardScaler()\n",
        "        self.X_train = self.scaler_.fit_transform(X_train)\n",
        "        self.X_test = self.scaler_.transform(X_test)\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        if is_train:\n",
        "            self.X = self.X_train\n",
        "            self.y = self.y_train\n",
        "        else:\n",
        "            self.X = self.X_test\n",
        "            self.y = self.y_test\n",
        " \n",
        "        self.X = torch.FloatTensor(self.X)\n",
        "        self.y = torch.FloatTensor(self.y)\n",
        " \n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        " \n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        " \n",
        "def train(model, optimizer, criterion, train_loader, test_loader, scheduler=None):\n",
        "    '''\n",
        "    params:\n",
        "        model - torch.nn.Module to be fitted\n",
        "        optimizer - model optimizer\n",
        "        criterion - loss function from torch.nn\n",
        "        train_loader - torch.utils.data.Dataloader with train set\n",
        "        test_loader - torch.utils.data.Dataloader with test set\n",
        "                      (if you wish to validate during training)\n",
        "    '''\n",
        "    mean_test_loss_history = []\n",
        "    mean_train_loss_history = []\n",
        "    for epoch in range(400):\n",
        "        train_loss = []\n",
        "        test_loss = []\n",
        "        for x_train, y_train in tqdm(train_loader):        # берем батч из трейн лоадера\n",
        "            y_pred = model(x_train)                        # делаем предсказания\n",
        "            loss = torch.sqrt(criterion(y_pred.flatten(), y_train))    # считаем лосс\n",
        "            loss.backward()                                # считаем градиенты обратным проходом\n",
        "            train_loss.append(loss.cpu().detach().numpy())\n",
        "            optimizer.step()                               # обновляем параметры сети\n",
        "            optimizer.zero_grad()                          # обнуляем посчитанные градиенты параметров\n",
        " \n",
        "        mean_train_loss_history.append(np.mean(train_loss))\n",
        " \n",
        "        with torch.no_grad():\n",
        "            for x_test, y_test in tqdm(test_loader):\n",
        "                y_pred = model(x_test)\n",
        "                loss = torch.sqrt(criterion(y_pred.flatten(), y_test))\n",
        "                test_loss.append(loss.cpu().numpy())\n",
        "        mean_test_loss_history.append(np.mean(test_loss))\n",
        " \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        " \n",
        "        clear_output(True)\n",
        " \n",
        "        plt.figure(figsize=(13, 7))\n",
        "        plt.title(\"Ошибка на тесте и на трейне\")\n",
        "        plt.ylim((0, 23))\n",
        "        plt.xlabel(\"Эпоха\")\n",
        "        plt.ylabel(\"RMSE\")\n",
        "        plt.plot(np.arange(1, epoch + 2), mean_train_loss_history, color='b', label='RMSE на трейне')\n",
        "        plt.plot(np.arange(1, epoch + 2), mean_test_loss_history, color='r', label='RMSE на тесте')\n",
        "        plt.scatter(epoch + 1, mean_train_loss_history[-1], s=8, color='b', label='Трейн RMSE={:.2f}'.format(float(mean_train_loss_history[-1])))\n",
        "        plt.scatter(epoch + 1, mean_test_loss_history[-1], s=8, color='r', label='Тест RMSE={:.2f}'.format(float(mean_test_loss_history[-1])))\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        " \n",
        "def get_init_weights(activation):\n",
        "    assert activation in ['relu', 'elu', 'leaky_relu']\n",
        "    if activation in ['relu', 'elu']:\n",
        "        nonlinearity = 'relu'\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity=nonlinearity)\n",
        "            m.bias.data.fill_(0.0)\n",
        "    return init_weights\n",
        " \n",
        "class YearPredictor(nn.Module):\n",
        "    def __init__(self, hidden_layers, activation='elu', use_dropout=False, **kwargs):\n",
        "        super(self.__class__, self).__init__()\n",
        "        activation_mapping = {\n",
        "            'relu' : nn.ReLU,\n",
        "            'elu' : nn.ELU,\n",
        "            'leaky_relu' : nn.LeakyReLU\n",
        "        }\n",
        "        assert activation in activation_mapping.keys()\n",
        "        layers = []\n",
        "        layers.append(nn.BatchNorm1d(hidden_layers[0]))\n",
        "        layers.append(nn.Linear(hidden_layers[0], hidden_layers[1]))\n",
        " \n",
        "        for i in range(len(hidden_layers) - 2):\n",
        "            layers.append(activation_mapping[activation]())\n",
        "            layers.append(nn.BatchNorm1d(hidden_layers[i + 1]))\n",
        "            if use_dropout:\n",
        "                layers.append(nn.Dropout(0.5))\n",
        "            layers.append(nn.Linear(hidden_layers[i + 1], hidden_layers[i + 2]))\n",
        "        \n",
        "        self.model_ = nn.Sequential(*layers)\n",
        "        self.model_.apply(get_init_weights(activation))\n",
        " \n",
        "    def forward(self, inp):\n",
        "        return self.model_(inp)\n",
        " \n",
        "model = YearPredictor([90, 100, 40, 8, 1], use_dropout=False)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
        "criterion = nn.MSELoss()\n",
        " \n",
        "train_set = BaseDataset(X_train, y_train, X_test, y_test, is_train=True)\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        " \n",
        "test_set = BaseDataset(X_train, y_train, X_test, y_test, is_train=False)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=True)\n",
        " \n",
        " \n",
        " \n",
        "train(\n",
        "    model,\n",
        "    optimizer,\n",
        "    criterion, \n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    scheduler=scheduler\n",
        ")\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwUAAAG6CAYAAABZZurwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXQUdb7//9ebJCwR2ZElbFEEwhogMqCiiLI4X0SvMgI/9II6o3e8o1e+g7vOoKNzxnG9LjPq9+rgHRngCnPFZS6Dcl1wFzBqNCw6gGwOqwwIIQuf3x/V3XRCp7NApVOp5+OcOl1Ld9WnO5WkXvX+VLU55wQAAAAgvBqlugEAAAAAUotQAAAAAIQcoQAAAAAIOUIBAAAAEHKEAgAAACDkCAUA4AMzy0h1GwAAqC5CAQAcB2aWa2Z/NrMNZrZX0v9NdZtQ/5jZD82sn5k1NbOfpbo9ABBFKADQIJnZDDP73MwOmNm3ZvZ7M2vl07ZOkbRM0kuSejnnWjrn7vNjWwi8f0h6UdJmSZ1T3BYAiDG+vAxAQ2NmP5d0k6Tp8g7WsyT9TlJ7SWc454qP8/bmSCokCAAAgopKAYAGxcxaSLpL0nXOuSXOuRLn3AZJl0rqIemyyPNmm9nzca973sxmx02/Y2bR584ws3filt1kZs7MzovMGiapn5ltNrMdZvZHM2sZeW6PyHPTI9PXmtkXZtY2Mn2FmRWa2T4z+5uZXZPkvZVrR2TeZjMbFRkfZmbvm9l3ZrbNzB43s8aVrOtTM9tvZgfN7HBkfL+Z3RZZ3sfMXjOz3Wa2xswujXttMzN70Mw2mtneyGfV7FjWmaB9b5rZjyPjjSJVn81Jnu/MrGfc9D2RsBadfiFSMdprZm+bWb9K1nNTXLsPR97LfjP7IrJ8jpk9GXkf+8zsLTPrHvf6ZJ/bHDO7J276LxX2jdh7jkyfZ2Yb4qY7m9miyD623syur+zzAICaIhQAaGhOl9RU0p/jZzrn9kv6i6Qxx7JyM2sj6XpJ38XNzoxs9yxJ2ZJOkPR4gtdOkTRL0jjn3K7I7O2SJkhqIekKSQ+b2ZBaNq9M0kxJ7SSNkHSupGsTPdE5N8g511zS+ZK2OueaR4Zfm9kJkl6T9CdJJ0maIul3ZtY38vIHJA2NvOc28qoyh49xnclMl9S6xp9Gef8j6dTItldJmpvoSc6530bbLekbSRdEpuNDxDRJv5L3OedH11WT92hm50gaWN3Gm1kjSS9L+lRe5etcSTeY2bjqrgMAkiEUAGho2kna6ZwrTbBsW2T5sbhN0rOS9laY/5Bz7m+R8HGrpCnRM8AR4yU9I+l851zsjLdz7lXn3NfO85akpZJG1qZhzrmVzrkPnHOlkerIU5LOrsWqJkja4Jz7Q2Rdn0haJOlHkYPTKyX9m3Nui3OuzDn3nnPuUG3XmexFZtZU0i/kHYTXmnPuWefcvkg7Z0saFK3m1MKrzrm3I+u6XdIIM+uqar5HMzNJv5X3vqrrNEntnXN3O+eKnXN/k/T/5AUPADhm6VU/BQACZaekdmaWniAYdIosr5VIN5FLJfWT9M9xiw5J2hg3vVHe39cOcfP+Q9IGeQfpa+LWeb6kX0rqJe9ETaakz5M0Y7iZxVcpWsStq5ekhyTlRdaTLmlltd5ced0l/aDCdtIl/VFeqGoq6evjuM5k/k3SEsV9ZkmsMrPDkfGmkuZLkpmlSbpX3sF5e0nR57TT0eGuOjZFR5xz+81st7yLhqv7Hi+Vtx/+b4J1P2pmD8S9Nrq/dpfUucK60yQtr0X7AeAoVAoANDTvyztIvzh+pplFu7UsO4Z1/0rSb51z+yrM/0beQVtUN0mlkv4eN2+qpMmS7jWzLpE2NZF3JvkBSR2cc63kdXGyJG34wDnXKjpI2hq37PeSVks61TnXQl5VI9m6KrNJ0lvx24l0ofmpvIPUIkmnHMd1VqaNpJ/Ju0akOobEfS4PxM3//yRdKOk8SS3lXVsi1e6zkaSu0ZHIftVG3s+hOu8xQ95+dHMl674+7j1cFDd/k6T1FdZ9onPuh7V8DwBQDqEAQIPinNsr7yDyMTMbb2YZZtZD0n/Juw1kVWemK9NT0g/kdcmpaJ6kmWaWHTlI/LWkBRUqFcudcwWSHpX0dGReY0lNJO2QVBqpGoytZfsk6UR5t7zcb2Z9JCU74E7mFUm9zOzyyOeXYWanmVmOc+6wvO5TD0UufE0zsxGRgFOrdSZ5zQ2SnnHOfVvL9xF1oryguEteBeXXx7i+H5rZmeZdxP0reUFtk6r3Hi+X9J5z7rMabvMjSfvM7ObIRd1pZtbfzE47xvcCAJIIBQAaIOfcb+WdJX9A3kHyh/LOtJ5boe/7P5l3957Nkv5J0v+Nm654sNVB0h3OuZIEm5wj6XlJb0taL+9MemVfTPUbSZ3MbHqk4nC9vMCyR94Z7Zdq+n7jzIqsY5+8/uYLarOSSLvGyuuvvlXSt5Lukxdgotv5XNLHknZHliX9f1KNdSaSpvJn/GvrP+V16doi6UtJHxzj+v4kr8vXbnkXXF8mVfs9tpZ0Z0036Jwrk3fNQq68fWynvC5ptb0uAgDK4XsKAACopshtTjc75+5IdVsA4HiiUgAAAACEHKEAAAAACDm6DwEAAAAhR6UAAAAACDlCAQAAABBygfhG43bt2rkePXqkuhnVVlQkffGFdPLJUuvWqW4NAAAAIK1cuXKnc659omWBCAU9evTQihUrUt2Malu9WsrJke65R5o6NdWtAQAAACQz21jZMroP+SAtzXs8fDi17QAAAACqg1Dgg0aRT7WsLLXtAAAAAKqDUOADKgUAAAAIkkBcUxA0VAoAAEAqlZSUaPPmzSoqKkp1U5ACTZs2VZcuXZSRkVHt1xAKfEClAAAApNLmzZt14oknqkePHjKzVDcHdcg5p127dmnz5s3Kzs6u9uvoPuQDKgUAACCVioqK1LZtWwJBCJmZ2rZtW+MqEaHAB9FKAaEAAACkCoEgvGrzsycU+IDuQwAAIOzS0tKUm5ur/v3764ILLtB3330nSdqwYYPMTHfccUfsuTt37lRGRoZ+9rOfSZLWrFmjUaNGKTc3Vzk5Obr66qslSW+++aZatmyp3Nzc2PD666/X/ZurpkOHDumCCy5QXl6ebrrpplQ3JymuKfAB3YcAAEDYNWvWTPn5+ZKk6dOn64knntDtt98uScrOztarr76qe+65R5L0wgsvqF+/frHXXn/99Zo5c6YuvPBCSdLnn38eWzZy5Ei98sordfU2jkmTJk308ssvp7oZ1UKlwAdUCgAAAI4YMWKEtmzZEpvOzMxUTk6OVqxYIUlasGCBLr300tjybdu2qUuXLrHpAQMG1Gh7zZs3j42vWLFCo0aNkiR99NFHGjFihAYPHqzTTz9da9asOeq106ZNU25urtq0aaPs7Gzl5ubqySef1Jw5c3ThhRdq1KhROvXUU3XXXXfFXvP8889r2LBhys3N1TXXXKOyyJnh+HaMHDlSEyZMkCTNnj1bDzzwQGzZhAkT9Oabb0qSli5dqhEjRmjIkCH60Y9+pP3799fovdcWocAHVAoAAAA8ZWVlWrZsmSZOnFhu/pQpUzR//nxt2rRJaWlp6ty5c2zZzJkzNXr0aJ1//vl6+OGHY12PJGn58uXlug99/fXX1W5Lnz59tHz5cn3yySe6++67ddtttx31nLlz5yo/P18TJ07U/fffr/z8fP3Lv/yLJC9ULFq0SJ999pleeOEFrVixQoWFhVqwYIHeffdd5efnKy0tTXPnzi23zldffVV79+6tsn07d+7UPffco9dff12rVq1SXl6eHnrooWq/v2NB9yEfUCkAAAD1xQ03SJFePMdNbq70yCPJn3Pw4EHl5uZqy5YtysnJ0ZgxY8otHz9+vO6880516NBBkydPLrfsiiuu0Lhx47RkyRItXrxYTz31lD799FNJ1es+FN12dLxTp06SpL1792r69Olat26dzEwlJSU1edsaM2aM2rZtK0m6+OKL9c477yg9PV0rV67UaaedFtveSSedFHuNc0733nuvbrvtNj3//POx+Q8//HBsev369Zo1a5Y++OADffnllzrjjDMkScXFxRoxYkSN2lhbVAp8QKUAAACEXfSago0bN8o5pyeeeKLc8saNG2vo0KF68MEHNWnSpKNe37lzZ1155ZVavHix0tPTVVBQUONt5+fnlztrf+edd+qcc85RQUGBXn755RrftrPiXX3MTM45TZ8+Pba9NWvWaPbs2bHnzJs3T6NGjVLHjh3LvXbmzJmx14wcOVKSFyDGjBkTm//ll1/qmWeeqVEba4tKgQ+oFAAAgPqiqjP6fsvMzNSjjz6qiy66SNdee225ZT//+c919tlnq02bNuXmL1myROeee64yMjL07bffateuXcrKytLq1auPqS179+5VVlaWJGnOnDk1fv1rr72m3bt3q1mzZnrxxRf17LPPKjMzUxdeeKFmzpypk046Sbt379a+ffvUvXt3HT58WI888oiWLl0au+g6meHDh+tf//Vf9dVXX6lnz576/vvvtWXLFvXq1avGba0pKgU+oFIAAABwxODBgzVw4EDNmzev3Px+/fpp+vTpRz1/6dKl6t+/vwYNGqRx48bp/vvvj51pr3hNwcKFC6vdjptuukm33nqrBg8erNLS0hq/j2HDhumSSy7RwIEDdckllygvL099+/bVPffco7Fjx2rgwIEaM2aMtm3bJsnrSnTJJZeoVatW1Vp/+/btNWfOHE2dOlUDBw7UiBEjjjkIVZc55+pkQ8ciLy/PRa9ODwoz6c47pbvvTnVLAABA2BQWFionJyfVzWhQ5syZoxUrVujxxx9PdVOqJdE+YGYrnXN5iZ5PpcAnaWl0HwIAAEAwcE2BTxo1ovsQAABAQzFjxgzNmDEj1c3wDZUCn1ApAAAAQFAQCnxCpQAAAABBQSjwCZUCAAAABAWhwCdUCgAAABAUhAKfUCkAAABhlpaWptzcXPXv318XXHCBvvvuO0nShg0bZGa64447Ys/duXOnMjIy9LOf/UyStGbNGo0aNUq5ubnKycnR1VdfLUl688031bJly3LfU/D666/X/ZtrgAgFPqFSAAAAwqxZs2bKz89XQUGB2rRpoyeeeCK2LDs7W6+++mps+oUXXlC/fv1i09dff71mzpyp/Px8FRYW6rrrrostGzlypPLz82PDeeedVzdvqIEjFPgkLY1QAAAAIEkjRozQli1bYtOZmZnKyclR9MtpFyxYoEsvvTS2fNu2berSpUtsesCAATXaXvPmzWPjK1as0KhRoyRJH330kUaMGKHBgwfr9NNP15o1a4567bRp05Sbm6s2bdooOztbubm5evLJJ1VWVqYbb7xRp512mgYOHKinnnoq9pr77rtPAwYM0KBBg3TLLbfEvnW5b9++atasWayqIUkrV67U2WefraFDh2rcuHGxbz9ONb6nwCd0HwIAAJDKysq0bNkyXXXVVeXmT5kyRfPnz1eHDh2Ulpamzp07a+vWrZKkmTNnavTo0Tr99NM1duxYXXHFFWrVqpUkxQ64oxYtWqRTTjmlWm3p06ePli9frvT0dL3++uu67bbbtGjRonLPmTt3riTvewkmTJigSZMmSZKefvpptWzZUh9//LEOHTqkM844Q2PHjtXq1au1ePFiffjhh8rMzNTu3bvVpk0b5efna8OGDZowYYLy8/MlSSUlJbruuuu0ePFitW/fXgsWLNDtt9+uZ599thaf7PFFKPAJ3YcAAEC9cMMNUuSg9LjJzZUeeSTpUw4ePKjc3Fxt2bJFOTk5GjNmTLnl48eP15133qkOHTpo8uTJ5ZZdccUVGjdunJYsWaLFixfrqaee0qeffirJ6z70yiuvVGvb0fFOnTpJkvbu3avp06dr3bp1MjOVlJRU+y0vXbpUn332mRYuXBhb17p16/T666/riiuuUGZmpiSpTZs2la5jzZo1KigoiH0WZWVlsbalGt2HfEKlAAAAhFn0moKNGzfKOVfumgJJaty4sYYOHaoHH3wwdjY+XufOnXXllVdq8eLFSk9PV0FBQY23nZ+fHzvzL0l33nmnzjnnHBUUFOjll19WUVFRtdfpnNNjjz0WW+/69es1duzYar8+uo5+/frF1vH5559r6dKlNVqHX6gU+IRKAQAAqBeqOKPvt8zMTD366KO66KKLdO2115Zb9vOf/1xnn332UWfXlyxZonPPPVcZGRn69ttvtWvXLmVlZWn16tXH1Ja9e/cqKytLkjRnzpwavXbcuHH6/e9/r9GjRysjI0Nr165VVlaWxowZo7vvvlvTpk0r130okd69e2vHjh16//33NWLECJWUlGjt2rXlLrJOFSoFPqFSAAAA4Bk8eLAGDhyoefPmlZvfr18/TZ8+/ajnL126VP3799egQYM0btw43X///erYsaOkI9cURIdod57quOmmm3Trrbdq8ODBKi0trdF7+PGPf6y+fftqyJAh6t+/v6655hqVlpZq/PjxmjhxovLy8pSbm6sHHnig0nU0btxYCxcu1M0336xBgwYpNzdX7733Xo3a4RdzzqW6DVXKy8tz0avTg6J3b2nwYGn+/FS3BAAAhE1hYaFycnJS3QykUKJ9wMxWOufyEj2fSoFPuCUpAAAAgoJQ4BO6DwEAACAoCAU+4UJjAAAABAWhwCdUCgAAABAUhAKfUCkAAABAUBAKfEKlAAAAAEFBKPAJlQIAABBWu3btin2PQMeOHZWVlRWbLi4urvH6br75Zp122mmaMGGCDh06VKPXzp49O7b9vn37lvuuhBkzZigzM1P79u2LzbvhhhtkZtq5c6ck6d5771W/fv00cOBA5ebm6sMPP5QkjRo1Sr179469r0TfylyZm266Sf369VNOTo6uv/56JfuKgAcffLBce5xzuv7669WzZ08NHDhQq1atqtHnURm+0dgnVAoAAEBYtW3bVvn5+ZK8g/LmzZtr1qxZtV7ffffdd0ztmTlzpmbNmqV169Zp6NChmjRpkjIyMiRJPXv21OLFi3XZZZfp8OHD+t///d/Ytx6///77euWVV7Rq1So1adJEO3fuLBdq5s6dq7y8hLf9r9R7772nd999V5999pkk6cwzz9Rbb72lUaNGHfXcTZs2aenSperWrVts3v/8z/9o3bp1WrdunT788EP99Kc/jQWVY0GlwCdUCgAAAMrbsGGD+vTpo2nTpiknJ0eTJk3SgQMHJEkrV67U2WefraFDh2rcuHHatm2bJO+MfPRLbO+44w41b95ckvTmm29qwoQJsXU/8MADmj17dtLtn3rqqcrMzNSePXti86ZMmaIFCxbE1nnGGWcoPd07b75t2za1a9dOTZo0kSS1a9dOnTt3PqbPwMxUVFSk4uJiHTp0SCUlJerQoUPC586cOVO//e1vZWaxeYsXL9Y///M/y8w0fPhwfffdd7HP6lgQCnzCl5cBAAAcbc2aNbr22mtVWFioFi1a6He/+51KSkp03XXXaeHChVq5cqWuvPJK3X777eVet337di1btuyYtr1q1SqdeuqpOumkk2LzevXqpR07dmjPnj2aN2+epkyZEls2duxYbdq0Sb169dK1116rt956q9z6pk2bFus+dOONN0ryqgfRefFDtHvRiBEjdM4556hTp07q1KmTxo0bl/DbpxcvXqysrCwNGjSo3PwtW7aoa9eusekuXbpoy5Ytx/S5SHQf8k1amlRSkupWAAAAVM/atdLy5dLIkVKvXv5tp2vXrjrjjDMkSZdddpkeffRRjR8/XgUFBRozZowkqaysTJ06dSr3ul/96le67bbbNHXq1Ni85cuXKzc3V5K0Y8cO/eQnP0m4zYcfflh/+MMftHbtWr388stHLb/44os1f/58ffjhh3rqqadi85s3b66VK1dq+fLleuONNzR58mT95je/0YwZMyQl7j40bdo0TZs2rdL3/9VXX6mwsFCbN2+WJI0ZM0bLly/XyJEjY885cOCAfv3rX2vp0qWVrud4IxT4hO5DAAAgKNaulYYMkZyTzKRVq/wLBvFdYaLTzjn169dP77//fsLXbNiwQQUFBXrsscfKzR85cqReeeUVSV73of379yd8ffSagpdeeklXXXWVvv76azVt2jS2fPLkyRo6dKimT5+uRo3Kd6RJS0vTqFGjNGrUKA0YMEDPPfdcLBQkMnfuXN1///1Hze/Zs6cWLlyo//7v/9bw4cNj3aDOP/98vf/+++VCwddff63169fHqgSbN2/WkCFD9NFHHykrK0ubNm2KPXfz5s2xayCOBd2HfMKFxgAAICiWL/cCwYED3uPy5f5t65tvvokd/P/pT3/SmWeeqd69e2vHjh2x+SUlJfriiy9ir7nrrrt01113HfO2J06cqLy8PD333HPl5nfv3l333nuvrr322nLz16xZo3Xr1sWm8/Pz1b1796TbmDZtmvLz848aFi5cKEnq1q2b3nrrLZWWlqqkpERvvfXWUd2HBgwYoO3bt2vDhg3asGGDunTpolWrVqljx46aOHGi/vM//1POOX3wwQdq2bLlUVWV2vAtFJhZVzN7w8y+NLMvzOzfIvPbmNlrZrYu8tjarzakEpUCAAAQFCNHehWCzEzvMe6k9XHXu3dvPfHEE8rJydGePXv005/+VI0bN9bChQt18803a9CgQcrNzdV7770Xe02XLl101llnHZft/+IXv9BDDz2kwxXO3l5zzTU65ZRTys3bv3+/pk+frr59+2rgwIH68ssvy13MHH9NwXnnnVet7U+aNEmnnHKKBgwYoEGDBmnQoEG64IILJEk//vGPYxdVV+aHP/yhTj75ZPXs2VM/+clP9Lvf/a5a262KJbsv6jGt2KyTpE7OuVVmdqKklZIukjRD0m7n3G/M7BZJrZ1zNydbV15enqvqA6pvJk6UNm/2ym8AAAB1qbCwMOHFq8nUxTUFGzZs0IQJE1RQUODPBhCTaB8ws5XOuYT3UPXtmgLn3DZJ2yLj+8ysUFKWpAsljYo87TlJb0pKGgqCiEoBAAAIkl69/L3AGPVbnVxTYGY9JA2W9KGkDpHAIEnfSkp8Y9aA45oCAACA8nr06EGVoJ7yPRSYWXNJiyTd4Jz7R/wy5/VdSth/ycyuNrMVZrZix44dfjfzuKNSAAAAgKDwNRSYWYa8QDDXOffnyOy/R643iF53sD3Ra51zTzvn8pxzee3bt/ezmb7gy8sAAAAQFH7efcgkPSOp0Dn3UNyilyRNj4xPl7TYrzakEt2HAAAAEBR+fnnZGZIul/S5meVH5t0m6TeS/svMrpK0UdKlPrYhZeg+BAAAgKDwrVLgnHvHOWfOuYHOudzI8Bfn3C7n3LnOuVOdc+c553b71YZUolIAAADCateuXbH793fs2FFZWVmx6eLi4uO6rTlz5qh9+/bKzc1Vnz599PDDD8eWzZ49W2amr776KjbvkUcekZnFvg/g2Wef1YABAzRw4ED1799fixd7nVhmzJih7OzsWLtPP/30arfp4YcfVr9+/dS/f39NnTpVRUVFRz1n5syZsXX36tVLrVq1ii1LS0uLLZs4cWKNP5Pa8LNSEGpUCgAAQFi1bdtW+fleR5HZs2erefPmmjVrlm/bmzx5sh5//HHt2rVLvXv31qRJk9S1a1dJ3rcDz58/X3fccYck6YUXXlC/fv0kSZs3b9a9996rVatWqWXLltq/f7/ib3Bz//33a9KkSTVqy5YtW/Too4/qyy+/VLNmzXTppZdq/vz5mjFjRrnnxYeXxx57TJ988klsulmzZrHPr67UyS1Jw4hKAQAAwNGef/55DRs2TLm5ubrmmmtUFjmLumTJEg0ZMkSDBg3Sueeeq4MHD8bOljdu3FgDBgxQbm5u0m/8bdu2rXr27Klt27bF5l100UWxs/9ff/21WrZsqXbt2kmStm/frhNPPFHNmzeXJDVv3lzZ2dnH/B5LS0t18OBBlZaW6sCBA+rcuXPS58+bN09Tp0495u0eC0KBT6gUAAAAlFdYWKgFCxbo3XffVX5+vtLS0jR37lzt2LFDP/nJT7Ro0SJ9+umneuGFF2Jny/Pz89W5c2e98cYbys/PV15ewi/klSR98803Kioq0sCBA2PzWrRooa5du6qgoEDz58/X5MmTY8sGDRqkDh06KDs7W1dccYVefvnlcuu78cYbY8Fk2rRpkqQ33ngjNi9+iHYvysrK0qxZs9StWzd16tRJLVu21NixYytt88aNG7V+/XqNHj06Nq+oqEh5eXkaPny4XnzxxZp9yLVE9yGfUCkAAACBsnattHy5NHKkb19tvGzZMq1cuVKnnXaaJOngwYM66aST9MEHH+iss86KnaVv06ZNjda7YMECvf3221q9erUef/xxNW3atNzyKVOmaP78+frrX/+qZcuW6Q9/+IMkr+/+kiVL9PHHH2vZsmWaOXOmVq5cqdmzZ0tK3H3onHPOSdq1Z8+ePVq8eLHWr1+vVq1a6Uc/+pGef/55XXbZZQmfP3/+fE2aNElpaWmxeRs3blRWVpb+9re/afTo0RowYIBOOeWUGn0mNUWlwCd8TwEAAAiMtWulIUOk66/3Hteu9WUzzjlNnz49VgFYs2ZN7AD8WEyePFmfffaZ3nvvPd1yyy369ttvyy2fMGGC/vjHP6pbt25q0aJFuWVmpmHDhunWW2/V/PnztWjRoqTbqqpS8Prrrys7O1vt27dXRkaGLr74Yr333nuVrm/+/PlHdR3KysqSJJ188skaNWpUuesN/EIo8AndhwAAQGAsXy45Jx044D0uX+7LZs4991wtXLhQ27d73127e/dubdy4UcOHD9fbb7+t9evXx+bXRl5eni6//HL9+7//e7n5mZmZuu+++3T77beXm79161atWrUqNp2fn6/u3bsn3Ua0UlBxiB74d+vWTR988IEOHDgg55yWLVumnJychOtavXq19uzZoxEjRsTm7dmzR4cOHZIk7dy5U++++6769u1b/Q+hlug+5BO6DwEAgMAYOVIykzIzvceRI33ZTN++fXXPPfdo7NixOnz4sDIyMvTEE09o+PDhevrpp3XxxRfr8OHDOumkk/Taa6/Vahs333yzhgwZottuu63c/ClTphz13DwW2EgAACAASURBVJKSEs2aNUtbt25V06ZN1b59ez355JOx5TfeeKPuueee2PRHH32kxo0bJ93+D37wA02aNElDhgxRenq6Bg8erKuvvlqS9Itf/EJ5eXmx24zOnz9fU6ZMkfedv57CwkJdc801atSokQ4fPqxbbrmlTkKBOed838ixysvLc8muNK+PZs2Sfv976fvvU90SAAAQNoWFhZWena5UHVxTgLqTaB8ws5XOuYRXalMp8AmVAgAAECi9ehEGQoxrCnzCNQUAAAAICkKBT6gUAAAAICgIBT6hUgAAAFIpCNeNwh+1+dkTCnwS/f4JqgUAAKCuNW3aVLt27SIYhJBzTrt27TrqC9yqwoXGPokPBY2IXgAAoA516dJFmzdv1o4dO1LdFKRA06ZN1aVLlxq9hlDgk2gQKCuT0vmUAQBAHcrIyFB2dnaqm4EA4Ry2T+g+BAAAgKAgFPgkvlIAAAAA1GeEAp9QKQAAAEBQEAp8QqUAAAAAQUEo8AmVAgAAAAQFocAnVAoAAAAQFIQCn0QrBYQCAAAA1HeEAp/QfQgAAABBQSjwCd2HAAAAEBSEAp9QKQAAAEBQEAp8QqUAAAAAQUEo8AmVAgAAAAQFocAnVAoAAAAQFIQCn1ApAAAAQFAQCnzC9xQAAAAgKAgFPqH7EAAAAIKCUOATug8BAAAgKAgFPqFSAAAAgKAgFPiESgEAAACCglDgEyoFAAAACApCgU+oFAAAACAoCAU+oVIAAACAoCAU+IRKAQAAAIKCUOATvrwMAAAAQUEo8AndhwAAABAUhAKf0H0IAAAAQUEo8AmVAgAAAAQFocAnVAoAAAAQFIQCn1ApAAAAQFAQCnxCpQAAAABBQSjwCZUCAAAABAWhwCd8TwEAAACCglDgE7oPAQAAICgIBT6h+xAAAACCglDgEyoFAAAACApCgU+oFAAAACAoCAU+oVIAAACAoCAU+IRKAQAAAIKCUOATKgUAAAAICkKBT/ieAgAAAAQFocAndB8CAABAUBAKfEL3IQAAAAQFocAnVAoAAAAQFIQCn1ApAAAAQFAQCnxCpQAAAABBQSjwCZUCAAAABAWhwCdUCgAAABAUhAKfREMBlQIAAADUd4QCH6WlUSkAAABA/Uco8FGjRoQCAAAA1H+EAh+lpdF9CAAAAPUfocBHVAoAAAAQBIQCH1EpAAAAQBAQCnxEpQAAAABBQCjwEZUCAAAABAGhwEdUCgAAABAEhAIf8T0FAAAACAJCgY/oPgQAAIAgIBT4iO5DAAAACALfQoGZPWtm282sIG7ebDPbYmb5keGHfm2/PqBSAAAAgCDws1IwR9L4BPMfds7lRoa/+Lj9lKNSAAAAgCDwLRQ4596WtNuv9QcBlQIAAAAEQSquKfiZmX0W6V7UOgXbrzNUCgAAABAEdR0Kfi/pFEm5krZJerCyJ5rZ1Wa2wsxW7Nixo67ad1xRKQAAAEAQ1GkocM793TlX5pw7LOn/SRqW5LlPO+fynHN57du3r7tGHkd8TwEAAACCoE5DgZl1ipv8J0kFlT23IaD7EAAAAIIg3a8Vm9k8SaMktTOzzZJ+KWmUmeVKcpI2SLrGr+3XB3QfAgAAQBD4Fgqcc1MTzH7Gr+3VR1QKAAAAEAR8o7GPqBQAAAAgCAgFPqJSAAAAgCAgFPiISgEAAACCgFDgIyoFAAAACAJCgY+oFAAAACAICAU+4svLAAAAEASEAh/RfQgAAABBQCjwEd2HAAAAEASEAh9RKQAAAEAQEAp8RKUAAAAAQUAo8BGVAgAAAAQBocBHVAoAAAAQBIQCH1EpAAAAQBAQCnxEpQAAAABBQCjwEV9eBgAAgCAgFPiI7kMAAAAIAkKBj+g+BAAAgCAgFPiISgEAAACCgFDgIyoFAAAACAJCgY+oFAAAACAICAU+olIAAACAICAU+IhbkgIAACAICAU+ovsQAAAAgoBQ4CO6DwEAACAICAU+olIAAACAICAU+IhKAQAAAIKAUOAjKgUAAAAIAkKBj6gUAAAAIAgIBT6iUgAAAIAgIBT4KC3Ne3Qute0AAAAAkiEU+CgaCqgWAAAAoD4jFPioUeTTJRQAAACgPiMU+ChaKeBiYwAAANRnhAIfUSkAAABAEBAKfESlAAAAAEFAKPARlQIAAAAEAaHAR1QKAAAAEASEAh9RKQAAAEAQEAp8RKUAAAAAQUAo8BFfXgYAAIAgIBT4iO5DAAAACIKkocDMRseNZ1dYdrFfjWoo6D4EAACAIKiqUvBA3PiiCsvuOM5taXCoFAAAACAIqgoFVsl4omlUQKUAAAAAQVBVKHCVjCeaRgVUCgAAABAE6VUsP9nMXpJXFYiOKzKdXfnLIFEpAAAAQDBUFQoujBt/oMKyitOogFuSAgAAIAiShgLn3Fvx02aWIam/pC3Oue1+NqwhiHYfolIAAACA+qyqW5I+aWb9IuMtJX0q6T8lfWJmU+ugfYFGpQAAAABBUNWFxiOdc19Exq+QtNY5N0DSUEk3+dqyBoALjQEAABAEVYWC4rjxMZJelCTn3Le+tagB4UJjAAAABEFVoeA7M5tgZoMlnSFpiSSZWbqkZn43LuioFAAAACAIqrr70DWSHpXUUdINcRWCcyW96mfDGgIqBQAAAAiCqu4+tFbS+ATz/yrpr341qqGgUgAAAIAgSBoKzOzRZMudc9cf3+Y0LFQKAAAAEARVdR/6F0kFkv5L0lZ532SMauKWpAAAAAiCqkJBJ0k/kjRZUqmkBZIWOue+87thDQHdhwAAABAESe8+5Jzb5Zx70jl3jrzvKWgl6Uszu7xOWhdwdB8CAABAEFRVKZAkmdkQSVPlfVfB/0ha6WejGgoqBQAAAAiCqi40vlvS/5FUKGm+pFudc6V10bCGgEoBAAAAgqCqSsEdktZLGhQZfm1mknfBsXPODfS3ecFGpQAAAABBUFUoyK6TVjRQVAoAAAAQBFV9ednGRPPNrJG8awwSLoeHSgEAAACCIOndh8yshZndamaPm9lY81wn6W+SLq2bJgYXlQIAAAAEQVXdh/4oaY+k9yX9WNJt8q4nuMg5l+9z2wKPLy8DAABAEFQVCk52zg2QJDP7D0nbJHVzzhX53rIGgO5DAAAACIKk3YcklURHnHNlkjYTCKqP7kMAAAAIgqoqBYPM7B+RcZPULDIdvSVpC19bF3BUCgAAABAEVd19KK2uGtIQUSkAAABAEFTVfQjHgEoBAAAAgoBQ4CMqBQAAAAgCQoGPuCUpAAAAgoBQ4KNo9yEqBQAAAKjPfAsFZvasmW03s4K4eW3M7DUzWxd5bO3X9usDKgUAAAAIAj8rBXMkja8w7xZJy5xzp0paFplusLjQGAAAAEHgWyhwzr0taXeF2RdKei4y/pyki/zafn3AhcYAAAAIgrq+pqCDc25bZPxbSR3qePt1ikoBAAAAgiBlFxo755wkV9lyM7vazFaY2YodO3bUYcuOHyoFAAAACIK6DgV/N7NOkhR53F7ZE51zTzvn8pxzee3bt6+zBh5PVAoAAAAQBHUdCl6SND0yPl3S4jrefp3ilqQAAAAIAj9vSTpP0vuSepvZZjO7StJvJI0xs3WSzotMN2hpaVQKAAAAUL+l+7Vi59zUShad69c266NGjQgFAAAAqN/4RmOfpaXRfQgAAAD1G6HAZ1QKAAAAUN8RCnxGpQAAAAD1HaHAZ1QKAAAAUN8RCnxGpQAAAAD1HaHAZ1QKAAAAUN8RCnxGpQAAAAD1nW/fUxAqhw5Ju3YlHH65f7dO+OJUae9kqWXLVLcUAAAAOAqhoDr+8Adpw4YjB/s7d5Y/+P/++0pfeoWaqOk7h6RON0iTJklXXimdfbZkVnftBwAAAJIgFFTHI49In38utWoltW0rtWsnde4sDRjgTScZ+vRpqisGrNAvuzwjzZsn/fGP0imneOFg+nQpKyvV7y68DhyQPvxQev99qWtX6cILpRYtUt0qAACAOmfOuVS3oUp5eXluxYoVqWvAP/4hnXCCd4FADZ18snTGGV4W0IED0qJF0rPPSm++6V2FPH68FxAuuEBq3PjY2rhypfTxx9JHH3njmZnSaacdGQYNkpo0qf02gm7PHundd6Xly6W33/Y+o5KSI8ubNJHOP1+aMkWaMMH7mddHe/ZI77wjnXiilJvrhVUAAIAqmNlK51xewmWEAn+deqo0bJg0d26FBV99Jc2Z4w1btnjVh8sv9wJC//7JV3rokPTZZ97BfzQErF4tRX+WJ58s5eV53Zo+/ljavt2bn5HhBYP4oJCTU6uwEwhbt3oBIDp8/rn3GWVkeO995EhvGDHC+/wWLJBeeEHats0LVBMmSJMne0GhWbPUvY+iIi/MvP66N6xceeRnLUnZ2dLgwV5AGDzYGzp3Pn5d1MrKvM9kwwbpm2+k9u29KlmHDnSDAwAgQAgFKZSTI23cKHXv7p3QjQ4tW3qPrVuUqf/WpRr48TPKWvWSGpWW6ODAYTp02VVqcvlkNTvpRGnNmvIB4NNPpeJibwMnneSljmHDvAPdvDwvYEQ5J23a5L02+voVK6R9+7zlJ5wgDR1aPihkZx/bwd7hw1JpqTeUlR0Zr848M++gPT29eo9pad5rnJO+/vpIFWD5cm86+h5HjJDOOssLAcOGeQf9iZSVea9dsEBauNC7fuTEE72uRZMnS2PHHltFpzrKyqRPPjkSAt55xwuC6ene+zj3XOmcc6SDB73nRYd1646so3378iEhN9dLqIkCYFmZF6A2bDh62LjRCwLxFZWotm29ADtggPfYv7/Urx+VCwAA6ilCQQotWiT99a/Sd9+VH/bu9XqBxB9rtdMOXabndZWeUX99oQNqplKlq4W8A/jv007U162GalPHYdrR4zR912uY0rO7qnUbU5s2UuvWij22bu0dQyZ0+LC0dm35oJCf7x14St7BXvv23vPKypIPiZ5T19LTva5Y0aDUtq105pleADjrLO+AOCOj5ustLZXeeMMLCH/+s/cDa9VK+qd/8roYjR6d5EOuAee8ylE0BLzxhrctyTvgPu88bzjrLKl588rXs2+fFxjz848EhYKCIzvZCSdIAwd61aLi4iMH/t98473XeB07Sj16HD107Sr9/e9e1aWg4MgQDZmS95xoSIgGhj59Ulttqc+Ki71KzNat3rBli/Ttt15wbdeu/NC+vfdLXpv9GbV3+LB3giD684n+jCr+3sSr6n9r8+be2aEWLRI/nnji8fn7AhwvznkHL998451s/OYbb/j2W2+Zmfe/ONljxXlpad4+H3+2NP4x+jvR6DjeQT96EjIj4/iuNyAIBfWUc17PkIqB4bs9To0//Vjd3/6jiosOa3WLYfq08Wn6orS3du5J05490u7d0v79ydfftKk3NGuWeDx++oSMYp18oEA993ys7jtWKLN0rxqlp6lRRpoaZTRSWkZkvHGa0iJDeuM0pTVJU3pGI6U3SVN6E2++ZaR7/8zS0rzH+CHZvLQ070MpLfUOZOMfE82r+Jid7R049+lz/H/Ri4u9A/YFC6QXX/Su4WjXTrrkEql3b+850T941R0/fNjrCrRsmfeHVfIOqMeM8ULA6NFeF51jbXdhYfmKwmefeQec0QP97t3LH/h361azA3jnvPZHA0I0MBQWHglqjRp5F9i3bu39nKszRP9hVJyXaIj+g0m2PKqm402alP+lSTRU/MVq2tSrKO3efeRAMnrQH3/wv3Wrd7BZUVpa8oDdqlX5oBAfHNq29Z5z6JA3FBdXf7y42Hvv8b+T0cfqzGvWzDvYPeGEI0Oy6WbNyn/Whw55gTj6Ry46nmze3r3e/tyqlbd/xT9WNS8jw+tmGf1ZRA/4K45v3Zq4WlZVRbWy5c5VHRok731VDAwtWnj7Vvzf0pqMV/bzq2q8USPvM4juM4mG+H0qfigp8X4nMjO9oVmzI+OJhvjlTZse+b9Q0yF64ir6eTt39HSyZclUtTz6dyP+70dl402bevti/P5SWupVgw8e9K5HPHDgyHhl8w4divxDP8H77Kp6zMws/7+yuNjb5yse9McP8SeAJG/f6NjxyJcyxX+OlT3Gj0f3qWTMvJBcMTA0b37k9TX5Oxf/tzUjo/zPKn5INK9JE2//jP4uJgv2LVp4n3H057p2rdcTYeRIqVev5O/ZR4SCBqqkxAsR0f+T8f8vd+/2/tcVFXl/K4qKajZe293CzPt/Vd0hI6P8dFX/K6JD/N+06BAN/RWPBY97t/eiImnJEi8gvPxy0lvSVql1a+/g/7zzvG5BPXs2nH76paVeBSQaEr74wkuyVVWfKqtCHT6cfIj+o6k41KevFG/UyAt6nTt7Q1bWkfH46bZtvV/w6C2Qd+zwHqNDoukdO46EsMq2Hf+PrXHjo6cbN/Y+x+hnHt+1r7J58Y8HDtTs8zY78gu8f7/3xyeZFi2OLom2aOG9bs8e7w9i/GOiA/l4TZokPiBp3tz7WUR/HonGO3asfcXGOa/Ne/d6Jxhq+pioK2Zl43X9Pz49/eiDqIwM73OOHsgeOFC3bQqKaDgoKqp63z2e24zeSGXHjqP3l3btvBNFiYauXb2/Z8d6XWJxsbdvR7tRxI8nm7d//5GD+vi/afF/2yqbl56eOMQWFVUeeqPLDh70wlF1/vdHKyGZmV5FWPLakJ+fsmBAKECNOOf9rkQDQvxQnXnFxbUbor930f8ZVZ08qK5EJ5ErnnSuGE7ij5GSTTdNK9EJjQ4qI92pcYY3pKcrNp2R4ZSRLm883VsWHW/cWFL79mqSmZbwZER0CGF1s27E/+1LNh79ZUg0xKfpREPr1uUP/Dt08K9LiHPeP6ldu46k8/gdqS5uKBD9vL7/3hv27z8ynmzegQPegXj0QL/igX/r1t6Zt5p8dtED74pBIf7xH//wAlj8wX5WlndWsqGIBuP4wFBZ0Es2XlZW/g9goqFx4+r9wYqWySue9U40FBV566xYYa5qiK8uVuy2kmiobFllki07fPjIP7Xo34JE44nmFRcfqarEnyWLjieb17jxkc81+ntV3cfiYu/3IP6gv0uXyq+/g/f7sW/fkbCeLMivXOl11S4r835ejz0mXXVVSppNKEAgRaunyf5fRIfvv/eeX9XJ5EQnnKP/++J7UMQHlcqm4yvj0R5Mfkh04i3+GK/i/6/KpuPHo9dpx1drMjKqPy9Rpaey8fjp+P/R1ekBFD8vvhdRQymmAABCYO1aaciQI9derFpVLysFXMWEeis93TthF5STdtHukdGhuDj5eMVwUdMhUffXyqYrLoteilFU5J3EqKqd0Xn1RfTEYaJu0om6RSc6CVjdk4bR6nRtKknRAJMo5FQ1Xt1LLxIN8Z9BVZdtAAB81quXFwTqwTUFyRAKgOMk/nqKhig+TFQMNtWZLi6u+hKAyuZFKzzJurMne6zs2sLqXHNYUuIFp8qqRfHvL4iqChM1mVdZ97zqzKv4uSe6JjHRvOhXjyS7LjDZsqjqhOv46XjxlbiKj5Uti/Zyid5hOX6oal6i6/CpngH1XK9e9TYMRBEKAFRL9Kx5RgbdTBOJBoj4wJAo7FRnPDpU7NJdnaE2r6nqtYnmV5wXvaY1ejOQ6nTbix+vzt0LE82Tjmwz0TWC9ek6c79VdvOu+PHqVssSLa+qOpfshkc1OQlQcTpRkIwPlNWZl+yxsmWJPoNklyhUXFbT6mB0PNHfherMc+7or/KpyXjF4Bo/XtW8mgT5+Mfoeqr6fKt7aUhNhmi7kz1WNi/ZDe4qU09uPpQUoQAAjoOGXikKqrKy5DcVkZJff5NsWkp+UJFsWVlZ+TsuV7zzcrLpRDfmqu50TaplFZdXVo0rKko8P368ptcQRYfo5xUfJCsGyqrmJXsMwGWVCID4bp+JQqxz3r0NJO864xTefCgpQgEAoMFKSztysxagosoCR3yASvZVB1Utq02lsGJISlRVSDRPKh8e47/Kpzrj0c8j/rOp7ryaVPcqLkv2mVZnvLZDsm5/8Y8V58W3IVEYTTRdUOBVCaI3JFm+nFAAAABQb5gd6UIE+CV686HGjb19buTIVLcoMUIBAAAA4JOA3HyIUAAAAAD4KQA3HxJ3qQYAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACGXnoqNmtkGSfsklUkqdc7lpaIdAAAAAFIUCiLOcc7tTOH2AQAAAIjuQwAAAEDopSoUOElLzWylmV2dojYAAAAAUOq6D53pnNtiZidJes3MVjvn3o5/QiQsXC1J3bp1S0UbAQAAgFBISaXAObcl8rhd0n9LGpbgOU875/Kcc3nt27ev6yYCAAAAoVHnocDMTjCzE6PjksZKKqjrdgAAAADwpKL7UAdJ/21m0e3/yTm3JAXtAAAAAKAUhALn3N8kDarr7QIAAABIjFuSAgAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCjlAAAAAAhByhAAAAAAg5QgEAAAAQcoQCAAAAIOQIBQAAAEDIEQoAAACAkCMUAAAAACFHKAAAAABCLiWhwMzGm9kaM/vKzG5JRRsAAAAAeOo8FJhZmqQnJJ0vqa+kqWbWt67bAQAAAMCTikrBMElfOef+5pwrljRf0oUpaAcAAAAApSYUZEnaFDe9OTIPAAAAQAqkp7oBlTGzqyVdHZncb2ZrjsNq20naeRzWA8Rjv4If2K/gB/YrHG/sU8HSvbIFqQgFWyR1jZvuEplXjnPuaUlPH88Nm9kK51ze8VwnwH4FP7BfwQ/sVzje2KcajlR0H/pY0qlmlm1mjSVNkfRSCtoBAAAAQCmoFDjnSs3sZ5L+KilN0rPOuS/quh0AAAAAPCm5psA59xdJf0nBpo9rdyQggv0KfmC/gh/Yr3C8sU81EOacS3UbAAAAAKRQSr7RGAAAAED9EYpQYGbjzWyNmX1lZrekuj0ILjN71sy2m1lB3Lw2Zvaama2LPLZOZRsRLGbW1czeMLMvzewLM/u3yHz2K9SamTU1s4/M7NPIfnVXZH62mX0Y+X+4IHLDD6BGzCzNzD4xs1ci0+xXDUCDDwVmlibpCUnnS+oraaqZ9U1tqxBgcySNrzDvFknLnHOnSloWmQaqq1TSz51zfSUNl/Svkb9R7Fc4FockjXbODZKUK2m8mQ2XdJ+kh51zPSXtkXRVCtuI4Po3SYVx0+xXDUCDDwWShkn6yjn3N+dcsaT5ki5McZsQUM65tyXtrjD7QknPRcafk3RRnTYKgeac2+acWxUZ3yfvH22W2K9wDJxnf2QyIzI4SaMlLYzMZ79CjZlZF0n/R9J/RKZN7FcNQhhCQZakTXHTmyPzgOOlg3NuW2T8W0kdUtkYBJeZ9ZA0WNKHYr/CMYp08ciXtF3Sa5K+lvSdc6408hT+H6I2HpF0k6TDkem2Yr9qEMIQCoA647zbeXFLL9SYmTWXtEjSDc65f8QvY79CbTjnypxzuZK6yKua90lxkxBwZjZB0nbn3MpUtwXHX0q+p6CObZHUNW66S2QecLz83cw6Oee2mVkneWflgGozswx5gWCuc+7PkdnsVzgunHPfmdkbkkZIamVm6ZGzuvw/RE2dIWmimf1QUlNJLST9u9ivGoQwVAo+lnRq5Mr4xpKmSHopxW1Cw/KSpOmR8emSFqewLQiYSH/cZyQVOuceilvEfoVaM7P2ZtYqMt5M0hh516u8IWlS5GnsV6gR59ytzrkuzrke8o6n/tc5N03sVw1CKL68LJJoH5GUJulZ59y9KW4SAsrM5kkaJamdpL9L+qWkFyX9l6RukjZKutQ5V/FiZCAhMztT0nJJn+tIH93b5F1XwH6FWjGzgfIu+EyTdwLwv5xzd5vZyfJuuNFG0ieSLnPOHUpdSxFUZjZK0izn3AT2q4YhFKEAAAAAQOXC0H0IAAAAQBKEAgAAACDkCAUAAABAyBEKAAAAgJAjFAAAAAAhRygAAEiSzOwHZvaGmX1qZoVm9nTkm5YBAA0cjnrV0gAAAhFJREFUoQAAENVU0uXOuUHu/2/v3lmriKIwDL8fpogWGoP4AywE8cJRtFTwUlhYia0kXv6C2HkpREFUxGAEbdQfYGNhYRNslDQhJqU2KogImlaEZZGdGESDUZMjnPeBAzNz9h726uZjbWaqtjD7vvF7XV6TJGkFGAokSQBU1VhVvV1wPgpsTnIqyUySifZ7l+QCQJJOkudJJpM8SrI+SV+S8fZxI5JcTnKpHZ9r/021TkS6UKok6QeGAknSvCRnFjz8TwCbgA/As6rqVFUHuLFgygPgbFXtYParzOer6iswDIwmOQQcBi628SNVtaeqtgGrgSMrU5kkaTGGAknSvKq6Ovfw3wLA5K/GJlkHDFTVWLt0H9jX7jMNPAQeAyer6ksbsz/JiyQvgQPA1uWqRZL0+/q6vQBJ0v8pyVqgA2z8w1tsBz7PzU/SD9wGdlfVm7YFqf8fLFWS9JfsFEiSAEgynGRnO14FXAOeAK9+Nr6qZoBPSfa2S8eBsTb/KDDIbOfgVpIBvgeAj+2tRseWqxZJ0tLYKZAkzZkGrrdtQYPAU+A0sGuROUPAnSRrgNfAiSQbgCvAwdYRGAFuVtVQkrvAFPAeGF/GWiRJS5Cq6vYaJEmSJHWR24ckSZKkHmcokCRJknqcoUCSJEnqcYYCSZIkqccZCiRJkqQeZyiQJEmSepyhQJIkSepxhgJJkiSpx30DKjDQLdXd2/YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 936x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2223fa592d6044fe8e58e9abd0770b21",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/14492 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-00575730b985>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-38-00575730b985>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_loader, test_loader, scheduler)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                \u001b[0;31m# считаем градиенты обратным проходом\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                               \u001b[0;31m# обновляем параметры сети\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                          \u001b[0;31m# обнуляем посчитанные градиенты параметров\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoEmjmikB8Ty",
        "outputId": "72610280-ad62-44b6-fc9a-57b964d9198c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "scheduler.last_epoch"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ4JR60YB8Cn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bine9EES6TIn"
      },
      "source": [
        "## Задание 2. (0 баллов, но при невыполнении максимум за все задание &mdash; 0 баллов)\n",
        "\n",
        "Напишите небольшой отчет о том, как вы добились полученного качества: какие средства использовали и какие эксперименты проводили. Подробно расскажите об архитектурах и значениях гиперпараметров, а также какие метрики на тесте они показывали. Чтобы отчет был зачтен, необходимо привести хотя бы 3 эксперимента."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk5pEwa66UZn"
      },
      "source": [
        "# YOUR CODE HERE (－.－)...zzzZZZzzzZZZ"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}